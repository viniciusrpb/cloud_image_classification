{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cloud_classification_ViT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSfvyfHsaa8fW99Oz+p+D+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusrpb/cloud_image_segmentation/blob/main/cloud_classification_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Green Microalga Classification using Vision Transformers"
      ],
      "metadata": {
        "id": "Ltlbw2CikP_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w7VyOGXbkNwJ"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r \"/content/drive/My Drive/img_satelite/classificacao/CCSN/train\" \"training\"\n",
        "#!cp -r \"/content/drive/My Drive/img_satelite/classificacao/CCSN/val\" \"validation\"\n",
        "#!cp -r \"/content/drive/My Drive/img_satelite/classificacao/CCSN/test\" \"testing\""
      ],
      "metadata": {
        "id": "68Nn5rzCkPY7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pytorch pytorch torchvision\n",
        "#!pip install timm==0.3.2\n",
        "#!pip install datasets transformers\n",
        "#!pip install transformers pytorch-lightning --quiet\n",
        "#!sudo apt -qq install git-lfs"
      ],
      "metadata": {
        "id": "sjjuPWctXrxL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import glob\n",
        "import pytorch_lightning as pl\n",
        "from huggingface_hub import HfApi, Repository\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchmetrics import Accuracy\n",
        "from transformers import ViTFeatureExtractor,ViTForImageClassification,DeiTForImageClassification,BeitForImageClassification,DeiTFeatureExtractor,  BeitFeatureExtractor\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "RP4M1K9DkWdq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = 'training'\n",
        "path_validation = 'validation'\n",
        "path_test = 'testing'"
      ],
      "metadata": {
        "id": "YGMhz6mNkWiS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the image generator objects"
      ],
      "metadata": {
        "id": "IvyUuPWd-0PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = torchvision.datasets.ImageFolder(path_train, transform=ToTensor())\n",
        "valid_ds = torchvision.datasets.ImageFolder(path_validation, transform=ToTensor())\n",
        "test_ds = torchvision.datasets.ImageFolder(path_test, transform=ToTensor())"
      ],
      "metadata": {
        "id": "51EmVl7C-0XH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUrAaBRehj8n",
        "outputId": "009139c3-5e0a-4273-f79e-886374d339d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ac', 'As', 'Cb', 'Cc', 'Ci', 'Cs', 'Ct', 'Cu', 'Ns', 'Sc', 'St']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_collator(batch):\n",
        "    encodings = feature_extractor([x[0] for x in batch], return_tensors='pt')\n",
        "    encodings['labels'] = torch.tensor([x[1] for x in batch], dtype=torch.long)\n",
        "    return encodings "
      ],
      "metadata": {
        "id": "OGchBzEU4o3M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pega os c√≥digos das classes do dataset"
      ],
      "metadata": {
        "id": "3AbZ5--E-9G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic_label2id = {}\n",
        "dic_id2label = {}\n",
        "for i, class_name in enumerate(train_ds.classes):\n",
        "  dic_label2id[class_name] = str(i)\n",
        "  dic_id2label[str(i)] = class_name"
      ],
      "metadata": {
        "id": "sLt3AWBxTzw-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allocate objects for loading the data using the DataGenerator"
      ],
      "metadata": {
        "id": "J78GiaITVegI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "\n",
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n"
      ],
      "metadata": {
        "id": "nHY1wvf9hGr7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\",\"f1-score\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
      ],
      "metadata": {
        "id": "raquc6rahEgJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(train_ds.classes),\n",
        "    id2label=dic_id2label,\n",
        "    label2id=dic_label2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMRqYXPBg9GP",
        "outputId": "2a93befe-b54e-4686-ec58-4a1731d14eaf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./vit-base-clouds\",\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=20,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=1e-5,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "7eC55oETldiD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=fn_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    tokenizer=feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "IaXAIE7slkQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34aca5f8-ed57-44cd-dab2-28c1400bed55"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "mgJoB1Tsl0Ku",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55487563-cd0d-437d-d12b-eb5ecee4928c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1774\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2220\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='415' max='2220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 415/2220 17:20 < 1:15:48, 0.40 it/s, Epoch 3.73/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.275400</td>\n",
              "      <td>2.238539</td>\n",
              "      <td>0.356000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.962900</td>\n",
              "      <td>2.021473</td>\n",
              "      <td>0.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.927900</td>\n",
              "      <td>1.862088</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.678700</td>\n",
              "      <td>1.753656</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-100\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-100/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-100/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-100/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-200] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-200\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-200/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-200/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-200/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-800] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-300\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-300/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-300/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-300/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-100] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-400\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-400/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-400/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-400/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-200] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2220' max='2220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2220/2220 1:33:02, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.275400</td>\n",
              "      <td>2.238539</td>\n",
              "      <td>0.356000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.962900</td>\n",
              "      <td>2.021473</td>\n",
              "      <td>0.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.927900</td>\n",
              "      <td>1.862088</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.678700</td>\n",
              "      <td>1.753656</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.517000</td>\n",
              "      <td>1.684789</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.443600</td>\n",
              "      <td>1.627982</td>\n",
              "      <td>0.508000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.284400</td>\n",
              "      <td>1.582013</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.225900</td>\n",
              "      <td>1.559297</td>\n",
              "      <td>0.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.083900</td>\n",
              "      <td>1.524139</td>\n",
              "      <td>0.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.085300</td>\n",
              "      <td>1.513997</td>\n",
              "      <td>0.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.007700</td>\n",
              "      <td>1.499365</td>\n",
              "      <td>0.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.955300</td>\n",
              "      <td>1.474194</td>\n",
              "      <td>0.532000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.942200</td>\n",
              "      <td>1.473687</td>\n",
              "      <td>0.560000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.816600</td>\n",
              "      <td>1.451723</td>\n",
              "      <td>0.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.756600</td>\n",
              "      <td>1.450550</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.778000</td>\n",
              "      <td>1.442011</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.704100</td>\n",
              "      <td>1.437505</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.684300</td>\n",
              "      <td>1.433078</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.647700</td>\n",
              "      <td>1.435435</td>\n",
              "      <td>0.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.709300</td>\n",
              "      <td>1.437849</td>\n",
              "      <td>0.560000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.656400</td>\n",
              "      <td>1.434917</td>\n",
              "      <td>0.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.630500</td>\n",
              "      <td>1.434452</td>\n",
              "      <td>0.544000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-500\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-500/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-500/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-500/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-300] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-600\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-600/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-600/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-600/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-400] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-700\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-700/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-700/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-700/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-800\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-800/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-800/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-800/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-600] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-900\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-900/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-900/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-900/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-700] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1000\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1000/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1000/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1000/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-800] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1100\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1100/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1100/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1100/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-900] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1200\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1200/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1200/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1200/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-1000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1300\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1300/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1300/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1300/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-1100] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1400\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1400/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1400/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1400/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-1200] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1500\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1500/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1500/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1500/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-1300] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1600\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1600/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1600/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1600/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-1400] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1700\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1700/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1700/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1700/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-1500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1800\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1800/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1800/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1800/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-1600] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-1900\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-1900/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-1900/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-1900/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-1700] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-2000\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-2000/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-2000/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-2000/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-1900] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-2100\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-2100/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-2100/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-2100/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-2000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 250\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./vit-base-clouds/checkpoint-2200\n",
            "Configuration saved in ./vit-base-clouds/checkpoint-2200/config.json\n",
            "Model weights saved in ./vit-base-clouds/checkpoint-2200/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/checkpoint-2200/preprocessor_config.json\n",
            "Deleting older checkpoint [vit-base-clouds/checkpoint-2100] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./vit-base-clouds/checkpoint-1800 (score: 1.4330781698226929).\n",
            "Saving model checkpoint to ./vit-base-clouds\n",
            "Configuration saved in ./vit-base-clouds/config.json\n",
            "Model weights saved in ./vit-base-clouds/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base-clouds/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         20.0\n",
            "  total_flos               = 2560799541GF\n",
            "  train_loss               =       1.1512\n",
            "  train_runtime            =   1:33:05.07\n",
            "  train_samples_per_second =        6.353\n",
            "  train_steps_per_second   =        0.397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate(test_ds)\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ],
      "metadata": {
        "id": "5YV8Z5mtl4cH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "24d7240e-5cac-4ac1-86bd-3ad3873f9e4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 519\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [65/65 00:29]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =       20.0\n",
            "  eval_accuracy           =     0.5684\n",
            "  eval_loss               =     1.3881\n",
            "  eval_runtime            = 0:00:30.17\n",
            "  eval_samples_per_second =     17.202\n",
            "  eval_steps_per_second   =      2.154\n"
          ]
        }
      ]
    }
  ]
}