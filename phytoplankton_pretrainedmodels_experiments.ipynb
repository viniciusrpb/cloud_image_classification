{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusrpb/cloud_image_classification/blob/main/phytoplankton_pretrainedmodels_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados Resnet50 - Phytoplankton"
      ],
      "metadata": {
        "id": "jMbhqJ7-xnug"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf68PvvD_GXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7356b43b-400f-4cc6-dfaf-bb2762a5a5eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons\n",
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKFCjB-rEV99",
        "outputId": "eaec0f23-2242-43c8-876f-4d16edb8ebc4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (23.0)\n",
            "Collecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEXKKhya6T2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870e36b1-ec55-4f77-be4b-4b928628de3b"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense,GlobalAveragePooling2D ,MaxPooling2D,Activation,Flatten,Conv2D,BatchNormalization,Dropout\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_addons as tfa\n",
        "import keras_tuner\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voJBV5X_g0B6"
      },
      "source": [
        "def loadData():\n",
        "    !cp -r \"/content/drive/My Drive/plankton\" \"plankton\"\n",
        "    path_data = \"plankton\"\n",
        "\n",
        "    list_subfolders = os.listdir(path_data)\n",
        "    \n",
        "    list_subfolders.sort()\n",
        "\n",
        "    dataset_dict = {}\n",
        "\n",
        "    dataset_dict['filename'] = []\n",
        "    dataset_dict['label'] = []\n",
        "\n",
        "    for folder in list_subfolders:\n",
        "\n",
        "        list_images_path = os.listdir(path_data+\"/\"+folder)\n",
        "        \n",
        "        list_images_path.sort()\n",
        "\n",
        "        for image_name in list_images_path:\n",
        "\n",
        "            dataset_dict['filename'].append(folder+\"/\"+image_name)\n",
        "\n",
        "            dataset_dict['label'].append(folder)\n",
        "    \n",
        "    df = pd.DataFrame.from_dict(dataset_dict)\n",
        "    labels = list(df.columns)\n",
        "\n",
        "    return df,labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter optimization"
      ],
      "metadata": {
        "id": "CsftBVcNOjtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_PreTrainedModel_resnet50(hp):\n",
        "\n",
        "    num_labels=11\n",
        "\n",
        "    model = Sequential()\n",
        "    \n",
        "    pre_trained_model = ResNet50(input_shape=(224,224,3),include_top=False,pooling ='avg',weights='imagenet', classes=num_labels)\n",
        "\n",
        "    for layer in pre_trained_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.add(pre_trained_model)\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Choice('num_neurons',[32,64,128,256,1024,2048]),activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(hp.Choice('prob_dropout',[0.1,0.2,0.3,0.4])))\n",
        "    model.add(Dense(num_labels,activation='softmax'))\n",
        "\n",
        "    f1_score = tfa.metrics.F1Score(num_classes=num_labels, average='macro',threshold=0.5)\n",
        "\n",
        "    adam = Adam(hp.Choice('learning_rate',[0.1,0.01,0.001,0.005,0.0001,0.0005,0.00001,0.00005,0.00002,0.0002]))\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=[f1_score])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "NYR1cnUoOl1s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_PreTrainedModel_inceptionv3(hp):\n",
        "\n",
        "    num_labels=11\n",
        "\n",
        "    model = Sequential()\n",
        "    \n",
        "    pre_trained_model = InceptionV3(input_shape=(224,224,3),include_top=False,pooling ='avg',weights='imagenet', classes=num_labels)\n",
        "\n",
        "    for layer in pre_trained_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.add(pre_trained_model)\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Choice('num_neurons',[32,64,128,256,1024,2048]),activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(hp.Choice('prob_dropout',[0.1,0.2,0.3,0.4])))\n",
        "    model.add(Dense(num_labels,activation='softmax'))\n",
        "\n",
        "    f1_score = tfa.metrics.F1Score(num_classes=num_labels, average='macro',threshold=0.5)\n",
        "\n",
        "    adam = Adam(hp.Choice('learning_rate',[0.1,0.01,0.001,0.005,0.0001,0.0005,0.00001,0.00005,0.00002,0.0002]))\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=[f1_score])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "oU4YcQ88G5gu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_PreTrainedModel_vgg16(hp):\n",
        "\n",
        "    num_labels=11\n",
        "\n",
        "    model = Sequential()\n",
        "    \n",
        "    pre_trained_model = VGG16(input_shape=(224,224,3),include_top=False,pooling ='avg',weights='imagenet', classes=num_labels)\n",
        "\n",
        "    for layer in pre_trained_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.add(pre_trained_model)\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Choice('num_neurons',[32,64,128,256,1024,2048]),activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(hp.Choice('prob_dropout',[0.1,0.2,0.3,0.4])))\n",
        "    model.add(Dense(num_labels,activation='softmax'))\n",
        "\n",
        "    f1_score = tfa.metrics.F1Score(num_classes=num_labels, average='macro',threshold=0.5)\n",
        "\n",
        "    adam = Adam(hp.Choice('learning_rate',[0.1,0.01,0.001,0.005,0.0001,0.0005,0.00001,0.00005,0.00002,0.0002]))\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=[f1_score])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "QaahdsppGqan"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getPreTrainedModel(units,prob,lr,num_labels,model_name):\n",
        "\n",
        "    model = Sequential()\n",
        "    \n",
        "    if model_name == 'vgg16':\n",
        "        pre_trained_model = VGG16(input_shape=(224,224,3),include_top=False,pooling ='avg',weights='imagenet', classes=num_labels)\n",
        "    elif model_name == 'resnet50':\n",
        "        pre_trained_model = ResNet50(input_shape=(224,224,3),include_top=False,pooling ='avg',weights='imagenet', classes=num_labels)\n",
        "    else:\n",
        "        pre_trained_model = InceptionV3(input_shape=(224,224,3),include_top=False,pooling ='avg',weights='imagenet', classes=num_labels)\n",
        "\n",
        "    for layer in pre_trained_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.add(pre_trained_model)\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units,activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(prob))\n",
        "    model.add(Dense(num_labels,activation='softmax'))\n",
        "\n",
        "    f1_score = tfa.metrics.F1Score(num_classes=num_labels, average='macro',threshold=0.5)\n",
        "\n",
        "    adam = Adam(learning_rate=lr)\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=[f1_score])\n",
        "    \n",
        "    return model    "
      ],
      "metadata": {
        "id": "w9zplob4UW6E"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparameterOptimization(flag_da,num_labels,model,path_data):\n",
        "\n",
        "    train, test = train_test_split(df, test_size=0.4, random_state=42)\n",
        "\n",
        "    valid, test = train_test_split(test, test_size=0.5, random_state=42)\n",
        "\n",
        "    agnostic_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    if flag_da == True:\n",
        "        \n",
        "        train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                           rotation_range=10,\n",
        "                                           width_shift_range=0.2,\n",
        "                                           height_shift_range=0.2,\n",
        "                                           zoom_range=0.2,\n",
        "                                           horizontal_flip=True,\n",
        "                                           brightness_range=[0.2,1.2])\n",
        "\n",
        "        train_generator = train_datagen.flow_from_dataframe(train, directory = path_data,\n",
        "                                                        x_col = \"filename\", y_col = \"label\",\n",
        "                                                        batch_size=32,\n",
        "                                                        seed=42,\n",
        "                                                        class_mode = \"categorical\")\n",
        "    else:\n",
        "        train_generator = agnostic_datagen.flow_from_dataframe(train, directory = path_data,\n",
        "                                                        x_col = \"filename\", y_col = \"label\",\n",
        "                                                        batch_size=32,\n",
        "                                                        seed=42,\n",
        "                                                        class_mode = \"categorical\")   \n",
        "\n",
        "    valid_generator = agnostic_datagen.flow_from_dataframe(valid, directory = path_data,\n",
        "                                                        x_col = \"filename\", y_col = \"label\",\n",
        "                                                        batch_size=32,\n",
        "                                                        seed=42,\n",
        "                                                        class_mode = \"categorical\")\n",
        "\n",
        "    if model == 'resnet50':\n",
        "\n",
        "        tuner = keras_tuner.RandomSearch(\n",
        "            build_PreTrainedModel_resnet50,\n",
        "            objective='val_loss',\n",
        "            max_trials=5)\n",
        "    elif model == 'vgg16':\n",
        "\n",
        "        tuner = keras_tuner.RandomSearch(\n",
        "            build_PreTrainedModel_vgg16,\n",
        "            objective='val_loss',\n",
        "            max_trials=5)\n",
        "    else:\n",
        "\n",
        "        tuner = keras_tuner.RandomSearch(\n",
        "            build_PreTrainedModel_inceptionv3,\n",
        "            objective='val_loss',\n",
        "            max_trials=5)\n",
        "\n",
        "\n",
        "    stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    tuner.search(train_generator, epochs=100, validation_data=valid_generator)\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=5)[0]\n",
        "\n",
        "    string = \"The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is \"+str(best_hps.get('num_neurons'))+\", dropout is \"+str(best_hps.get('prob_dropout'))+\" and the optimal learning rate for the optimizer is \"+str(best_hps.get('learning_rate'))+\".\"\n",
        "\n",
        "    return best_hps,string"
      ],
      "metadata": {
        "id": "1JBghFgoPRI7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stratified K-Fold Experiments"
      ],
      "metadata": {
        "id": "d9xIzSbkPmj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kfoldExperiments(model,best_hps,flag_da,path_data,num_labels,epochs):\n",
        "\n",
        "    skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
        "\n",
        "    X = np.array(df['filename'])\n",
        "    y = np.array(df['label'])\n",
        "\n",
        "    activation_f = 'softmax'\n",
        "    lr = best_hps.get('learning_rate')\n",
        "    prob = best_hps.get('prob_dropout')\n",
        "    num_neurons = best_hps.get('num_neurons')\n",
        "\n",
        "    f1_score = tfa.metrics.F1Score(num_classes=num_labels, average='macro',threshold=0.5)\n",
        "\n",
        "    test_f1_score = []\n",
        "    test_precision_score = []\n",
        "    test_recall_score = []\n",
        "\n",
        "    matrices = []\n",
        "\n",
        "    trial = 1\n",
        "\n",
        "    predicted_targets = np.array([])\n",
        "    actual_targets = np.array([])\n",
        "\n",
        "    for train_ix, test_ix in skf.split(X,y):\n",
        "\n",
        "        val_f1score = []\n",
        "        val_loss = []\n",
        "        \n",
        "        train_list = []\n",
        "        test_list = []\n",
        "\n",
        "        for ind in train_ix:\n",
        "            train_list.append([X[ind],y[ind]])\n",
        "        \n",
        "        for ind in test_ix:\n",
        "            test_list.append([X[ind],y[ind]])\n",
        "\n",
        "        X_train = pd.DataFrame(train_list, columns =['filename','label'])\n",
        "        X_test = pd.DataFrame(test_list, columns =['filename','label'])\n",
        "\n",
        "        agnostic_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "        if flag_da == True:\n",
        "        \n",
        "            train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                            rotation_range=10,\n",
        "                                            width_shift_range=0.2,\n",
        "                                            height_shift_range=0.2,\n",
        "                                            zoom_range=0.2,\n",
        "                                            horizontal_flip=True,\n",
        "                                            brightness_range=[0.2,1.2])\n",
        "\n",
        "            train_generator = train_datagen.flow_from_dataframe(X_train, directory = path_data,\n",
        "                                                            x_col = \"filename\", y_col = \"label\",\n",
        "                                                            batch_size=32,\n",
        "                                                            seed=42,\n",
        "                                                            class_mode = \"categorical\")\n",
        "        else:\n",
        "            train_generator = agnostic_datagen.flow_from_dataframe(X_train, directory = path_data,\n",
        "                                                            x_col = \"filename\", y_col = \"label\",\n",
        "                                                            batch_size=32,\n",
        "                                                            seed=42,\n",
        "                                                            class_mode = \"categorical\")   \n",
        "\n",
        "        test_generator = agnostic_datagen.flow_from_dataframe(X_test, directory = path_data,\n",
        "                                                            x_col = \"filename\", y_col = \"label\",\n",
        "                                                            batch_size=32,\n",
        "                                                            seed=42,\n",
        "                                                            class_mode = \"categorical\")\n",
        "        \n",
        "        early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
        "\n",
        "        model = getPreTrainedModel(num_neurons,prob,lr,num_labels,model_name)\n",
        "\n",
        "        history_fine = model.fit(train_generator,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=32,\n",
        "                        callbacks=[early_stopping]\n",
        "                        )\n",
        "            \n",
        "        f1 = history_fine.history['f1_score']\n",
        "\n",
        "        loss = history_fine.history['loss']\n",
        "\n",
        "        y_true = test_generator.labels\n",
        "        y_prob = model.predict(test_generator)\n",
        "        y_pred = np.argmax(y_prob,axis=1)\n",
        "\n",
        "        test_recall_score.append(recall_score(y_true, y_pred, average='macro'))\n",
        "        test_precision_score.append(recall_score(y_true, y_pred, average='macro'))\n",
        "        test_f1_score.append(recall_score(y_true, y_pred, average='macro'))\n",
        "\n",
        "        matrices.append(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "        predicted_targets = np.append(predicted_targets, y_pred)\n",
        "        actual_targets = np.append(actual_targets, y_true)\n",
        "\n",
        "        del model\n",
        "\n",
        "    return test_f1_score,test_precision_score,test_recall_score,matrices,actual_targets,predicted_targets"
      ],
      "metadata": {
        "id": "e4kepIrLDjNw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeResults(experiment_name,string_besthps,test_f1_score,test_precision_score,test_recall_score,matrices):\n",
        "\n",
        "    string = ''+string_besthps+'\\n'\n",
        "\n",
        "    string += 'f1_folds = ['\n",
        "    for s in test_f1_score:\n",
        "        string += str(s)+','\n",
        "\n",
        "    string+= string[:-1]+']\\n'\n",
        "\n",
        "    string += 'precision_folds =['\n",
        "    for s in test_precision_score:\n",
        "        string += str(s)+','\n",
        "\n",
        "    string+= string[:-1]+']\\n'\n",
        "\n",
        "    string += 'recall_folds = ['\n",
        "    for s in test_recall_score:\n",
        "        string += str(s)+','\n",
        "\n",
        "    string+= string[:-1]+']\\n'\n",
        "\n",
        "    file_ans = open(experiment_name+'_metrics.txt','w')\n",
        "    file_ans.write(string)\n",
        "    file_ans.close()"
      ],
      "metadata": {
        "id": "MAcLTywaxab5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_confusion_matrix(cnf_matrix, classes, normalize=False, title='Confusion matrix'):\n",
        "    if normalize:\n",
        "        cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cnf_matrix.max() / 2.\n",
        "\n",
        "    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
        "        plt.text(j, i, format(cnf_matrix[i, j], fmt), horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    return cnf_matrix"
      ],
      "metadata": {
        "id": "n6RMlkh9mUYw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(predicted_labels_list, y_test_list,name):\n",
        "    cnf_matrix = confusion_matrix(y_test_list, predicted_labels_list)\n",
        "    np.set_printoptions(precision=2)\n",
        "\n",
        "    # Plot non-normalized confusion matrix\n",
        "    plt.figure()\n",
        "    generate_confusion_matrix(cnf_matrix, classes=t, title='Confusion matrix, without normalization')\n",
        "    plt.savefig(name+'_confmatrix'+'.png', bbox_inches='tight')\n",
        "\n",
        "    # Plot normalized confusion matrix\n",
        "    plt.figure()\n",
        "    generate_confusion_matrix(cnf_matrix, classes=labels, normalize=True, title='Normalized confusion matrix')\n",
        "    plt.savefig(name+'_normconfmatrix'+'.png', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "DSVWskqiTSK4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run All Experiments"
      ],
      "metadata": {
        "id": "5VsoMdcvr6I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag_da = [False,True]\n",
        "models = ['proposed','resnet50','vgg16','inception_v3']\n",
        "\n",
        "path_data = 'plankton'\n",
        "\n",
        "for aug in flag_da:\n",
        "    \n",
        "    for model in models:\n",
        "        if aug:\n",
        "            experiment_name = path_data+'_'+model+'_aug_'\n",
        "        else:\n",
        "            experiment_name = path_data+'_'+model+'_'\n",
        "            \n",
        "        df,labels = loadData()\n",
        "\n",
        "        num_labels = len(labels)\n",
        "\n",
        "        best_hypers,string_hps = hyperparameterOptimization(flag_da,num_labels,model,path_data)\n",
        "\n",
        "        f1s,precisions,recalls,matrices,y_true,y_pred = kfoldExperiments(model,best_hypers,path_data,flag_da,num_labels,25)\n",
        "\n",
        "        writeResults(experiment_name,best_hypers,f1s,precisions,recalls,matrices)\n",
        "\n",
        "        plot_confusion_matrix(y_pred, y_true,experiment_name)"
      ],
      "metadata": {
        "id": "T3q8mjSRmLPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966980cb-b882-4134-9533-6374096d5489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4 Complete [00h 43m 37s]\n",
            "val_loss: 0.22976617515087128\n",
            "\n",
            "Best val_loss So Far: 0.2258644700050354\n",
            "Total elapsed time: 03h 21m 40s\n",
            "\n",
            "Search: Running Trial #5\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "32                |64                |num_neurons\n",
            "0.1               |0.2               |prob_dropout\n",
            "0.01              |0.0001            |learning_rate\n",
            "\n",
            "Epoch 1/100\n",
            "134/134 [==============================] - 32s 185ms/step - loss: 0.5332 - f1_score: 0.6016 - val_loss: 0.3954 - val_f1_score: 0.7128\n",
            "Epoch 2/100\n",
            "134/134 [==============================] - 23s 174ms/step - loss: 0.2490 - f1_score: 0.8397 - val_loss: 0.2438 - val_f1_score: 0.8105\n",
            "Epoch 3/100\n",
            "134/134 [==============================] - 23s 174ms/step - loss: 0.1762 - f1_score: 0.8895 - val_loss: 0.2981 - val_f1_score: 0.8256\n",
            "Epoch 4/100\n",
            "134/134 [==============================] - 27s 203ms/step - loss: 0.1479 - f1_score: 0.9297 - val_loss: 0.3066 - val_f1_score: 0.8166\n",
            "Epoch 5/100\n",
            "134/134 [==============================] - 22s 163ms/step - loss: 0.1345 - f1_score: 0.9378 - val_loss: 0.4511 - val_f1_score: 0.7728\n",
            "Epoch 6/100\n",
            "134/134 [==============================] - 23s 170ms/step - loss: 0.1106 - f1_score: 0.9441 - val_loss: 0.2514 - val_f1_score: 0.8608\n",
            "Epoch 7/100\n",
            "134/134 [==============================] - 29s 213ms/step - loss: 0.0868 - f1_score: 0.9649 - val_loss: 0.4740 - val_f1_score: 0.8429\n",
            "Epoch 8/100\n",
            "134/134 [==============================] - 23s 175ms/step - loss: 0.0881 - f1_score: 0.9672 - val_loss: 0.3433 - val_f1_score: 0.8038\n",
            "Epoch 9/100\n",
            "134/134 [==============================] - 23s 171ms/step - loss: 0.0740 - f1_score: 0.9713 - val_loss: 0.4045 - val_f1_score: 0.8045\n",
            "Epoch 10/100\n",
            "134/134 [==============================] - 23s 168ms/step - loss: 0.0746 - f1_score: 0.9744 - val_loss: 0.3624 - val_f1_score: 0.8528\n",
            "Epoch 11/100\n",
            "134/134 [==============================] - 23s 168ms/step - loss: 0.0656 - f1_score: 0.9710 - val_loss: 0.4589 - val_f1_score: 0.8054\n",
            "Epoch 12/100\n",
            "134/134 [==============================] - 23s 173ms/step - loss: 0.0704 - f1_score: 0.9679 - val_loss: 0.3556 - val_f1_score: 0.8228\n",
            "Epoch 13/100\n",
            "134/134 [==============================] - 22s 162ms/step - loss: 0.0521 - f1_score: 0.9696 - val_loss: 0.4984 - val_f1_score: 0.7693\n",
            "Epoch 14/100\n",
            "134/134 [==============================] - 23s 171ms/step - loss: 0.0562 - f1_score: 0.9735 - val_loss: 0.3815 - val_f1_score: 0.8399\n",
            "Epoch 15/100\n",
            "134/134 [==============================] - 28s 210ms/step - loss: 0.0528 - f1_score: 0.9710 - val_loss: 0.4590 - val_f1_score: 0.8117\n",
            "Epoch 16/100\n",
            "134/134 [==============================] - 28s 207ms/step - loss: 0.0596 - f1_score: 0.9741 - val_loss: 0.5143 - val_f1_score: 0.7493\n",
            "Epoch 17/100\n",
            "134/134 [==============================] - 23s 170ms/step - loss: 0.0738 - f1_score: 0.9625 - val_loss: 0.4593 - val_f1_score: 0.8191\n",
            "Epoch 18/100\n",
            "134/134 [==============================] - 23s 170ms/step - loss: 0.0574 - f1_score: 0.9703 - val_loss: 0.5062 - val_f1_score: 0.8241\n",
            "Epoch 19/100\n",
            "134/134 [==============================] - 27s 202ms/step - loss: 0.0417 - f1_score: 0.9781 - val_loss: 0.4455 - val_f1_score: 0.8136\n",
            "Epoch 20/100\n",
            "134/134 [==============================] - 27s 205ms/step - loss: 0.0392 - f1_score: 0.9818 - val_loss: 0.4910 - val_f1_score: 0.8196\n",
            "Epoch 21/100\n",
            "134/134 [==============================] - 27s 200ms/step - loss: 0.0410 - f1_score: 0.9840 - val_loss: 0.4501 - val_f1_score: 0.8419\n",
            "Epoch 22/100\n",
            "134/134 [==============================] - 28s 211ms/step - loss: 0.0418 - f1_score: 0.9757 - val_loss: 0.4552 - val_f1_score: 0.8421\n",
            "Epoch 23/100\n",
            "134/134 [==============================] - 22s 163ms/step - loss: 0.0416 - f1_score: 0.9797 - val_loss: 0.5046 - val_f1_score: 0.8183\n",
            "Epoch 24/100\n",
            "134/134 [==============================] - 22s 162ms/step - loss: 0.0285 - f1_score: 0.9831 - val_loss: 0.4302 - val_f1_score: 0.8538\n",
            "Epoch 25/100\n",
            "134/134 [==============================] - 23s 168ms/step - loss: 0.0405 - f1_score: 0.9734 - val_loss: 0.4506 - val_f1_score: 0.8530\n",
            "Epoch 26/100\n",
            "134/134 [==============================] - 23s 173ms/step - loss: 0.0398 - f1_score: 0.9845 - val_loss: 0.4933 - val_f1_score: 0.8189\n",
            "Epoch 27/100\n",
            "134/134 [==============================] - 27s 203ms/step - loss: 0.0404 - f1_score: 0.9825 - val_loss: 0.5402 - val_f1_score: 0.8371\n",
            "Epoch 28/100\n",
            "134/134 [==============================] - 23s 168ms/step - loss: 0.0294 - f1_score: 0.9852 - val_loss: 0.4549 - val_f1_score: 0.8591\n",
            "Epoch 29/100\n",
            "134/134 [==============================] - 22s 165ms/step - loss: 0.0389 - f1_score: 0.9813 - val_loss: 0.5033 - val_f1_score: 0.8414\n",
            "Epoch 30/100\n",
            "134/134 [==============================] - 27s 204ms/step - loss: 0.0316 - f1_score: 0.9882 - val_loss: 0.5381 - val_f1_score: 0.8395\n",
            "Epoch 31/100\n",
            "134/134 [==============================] - 27s 200ms/step - loss: 0.0379 - f1_score: 0.9788 - val_loss: 0.5426 - val_f1_score: 0.8416\n",
            "Epoch 32/100\n",
            "134/134 [==============================] - 22s 163ms/step - loss: 0.0506 - f1_score: 0.9802 - val_loss: 0.5935 - val_f1_score: 0.8243\n",
            "Epoch 33/100\n",
            "134/134 [==============================] - 28s 207ms/step - loss: 0.0290 - f1_score: 0.9843 - val_loss: 0.4457 - val_f1_score: 0.8432\n",
            "Epoch 34/100\n",
            "134/134 [==============================] - 23s 169ms/step - loss: 0.0195 - f1_score: 0.9863 - val_loss: 0.4328 - val_f1_score: 0.8383\n",
            "Epoch 35/100\n",
            "134/134 [==============================] - 22s 167ms/step - loss: 0.0306 - f1_score: 0.9867 - val_loss: 0.5360 - val_f1_score: 0.8478\n",
            "Epoch 36/100\n",
            "134/134 [==============================] - 27s 203ms/step - loss: 0.0237 - f1_score: 0.9906 - val_loss: 0.5294 - val_f1_score: 0.8395\n",
            "Epoch 37/100\n",
            "134/134 [==============================] - 22s 165ms/step - loss: 0.0316 - f1_score: 0.9823 - val_loss: 0.4953 - val_f1_score: 0.8281\n",
            "Epoch 38/100\n",
            "134/134 [==============================] - 23s 169ms/step - loss: 0.0348 - f1_score: 0.9803 - val_loss: 0.6624 - val_f1_score: 0.8200\n",
            "Epoch 39/100\n",
            "134/134 [==============================] - 23s 168ms/step - loss: 0.0306 - f1_score: 0.9815 - val_loss: 0.5026 - val_f1_score: 0.8344\n",
            "Epoch 40/100\n",
            "134/134 [==============================] - 22s 164ms/step - loss: 0.0198 - f1_score: 0.9895 - val_loss: 0.5002 - val_f1_score: 0.8487\n",
            "Epoch 41/100\n",
            "134/134 [==============================] - 23s 173ms/step - loss: 0.0158 - f1_score: 0.9924 - val_loss: 0.5477 - val_f1_score: 0.8447\n",
            "Epoch 42/100\n",
            "134/134 [==============================] - 27s 201ms/step - loss: 0.0198 - f1_score: 0.9856 - val_loss: 0.5580 - val_f1_score: 0.8642\n",
            "Epoch 43/100\n",
            "134/134 [==============================] - 23s 169ms/step - loss: 0.0273 - f1_score: 0.9849 - val_loss: 0.5664 - val_f1_score: 0.8546\n",
            "Epoch 44/100\n",
            "134/134 [==============================] - 23s 169ms/step - loss: 0.0236 - f1_score: 0.9872 - val_loss: 0.5289 - val_f1_score: 0.8383\n",
            "Epoch 45/100\n",
            "134/134 [==============================] - 23s 168ms/step - loss: 0.0279 - f1_score: 0.9885 - val_loss: 0.4801 - val_f1_score: 0.8683\n",
            "Epoch 46/100\n",
            "134/134 [==============================] - 22s 164ms/step - loss: 0.0184 - f1_score: 0.9917 - val_loss: 0.5584 - val_f1_score: 0.8572\n",
            "Epoch 47/100\n",
            "134/134 [==============================] - 23s 170ms/step - loss: 0.0220 - f1_score: 0.9903 - val_loss: 0.5033 - val_f1_score: 0.8606\n",
            "Epoch 48/100\n",
            "134/134 [==============================] - 22s 166ms/step - loss: 0.0179 - f1_score: 0.9897 - val_loss: 0.5751 - val_f1_score: 0.8453\n",
            "Epoch 49/100\n",
            "134/134 [==============================] - 23s 172ms/step - loss: 0.0491 - f1_score: 0.9731 - val_loss: 0.5472 - val_f1_score: 0.8397\n",
            "Epoch 50/100\n",
            "134/134 [==============================] - ETA: 0s - loss: 0.0317 - f1_score: 0.9841"
          ]
        }
      ]
    }
  ]
}